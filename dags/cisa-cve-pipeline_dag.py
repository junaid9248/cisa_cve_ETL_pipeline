# Operator: These are the nodes in a DAG, they represent a task.
# Dependencies: These are specified relationships between operators.
# Tasks: In airflow, a task is the unit of work.
# Task instances: Execution of a task at a specific point in time ie: in a DagRun

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

from src.config import PYTHONPATH, GCLOUD_PROJECTNAME, IS_LOCAL
import logging 
import os
from datetime import timedelta
import pendulum as pend


GH_TOKEN = os.environ.get('GH_TOKEN') 
GCLOUD_BUCKETNAME = os.environ.get('GCLOUD_BUCKETNAME')
GOOGLE_APPLICATION_CREDENTIALS = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

local_tz = pend.timezone('America/Chicago')
start_date = pend.datetime(2025, 12, 30, tz=local_tz)
# Define the DAG default args
default_args = {
    'owner': 'junaid',
    'retries': 1,
    'start_date': start_date,
    'retry_delay': timedelta(seconds = 30),
    # Adding a buffer window for tasks especially raws extraction task so it does NOT time out!
    'execution_timeout': timedelta(hours=4),
    'max_active_runs': 1,
}

env_dict = {
    'IS_LOCAL': str(IS_LOCAL), 
    'PYTHONPATH': PYTHONPATH,
    'GH_TOKEN': GH_TOKEN, 
    'GCLOUD_BUCKETNAME': GCLOUD_BUCKETNAME, 
    'GOOGLE_APPLICATION_CREDENTIALS': GOOGLE_APPLICATION_CREDENTIALS, 
    'GCLOUD_PROJECTNAME': GCLOUD_PROJECTNAME,
}
early_years = ['1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020']
early_str = ','.join(early_years[:10])

new_years = ['2021','2022','2023','2024','2025']
new_str = ','.join(new_years)

# Defining the DAG
with DAG(
    dag_id='cve_elt_pipeline',
    schedule ='@daily',
    default_args= default_args,
    description='Extract raw cve jsons from CISA Vulnrichment gihtub repo into GCS bucket -> Transform into structured table formats -> Load into BigQuery-> perform quality checks'

) as dag:
    #2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025
    # Defining the TASKS using operators
    # TASK 1 and 2: EXTRACT
    logging.info("Setting up the extract task")
    extract_raw_jsons = BashOperator(
        task_id = 'cve_raws_extraction',
        bash_command=(
            'cd /opt/airflow/repo && '
            f'python main.py --cloud'
        ),
        env = env_dict
    )


    #1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010
    # TASK 3: TRANSFORM and LOAD
    logging.info("Setting up the transform and staging task")
    load_raw_json = BashOperator(
        task_id = 'cve_raws_loading',
        bash_command=(
            'cd /opt/airflow/repo ; '
            'python -m src.load_raws_bq'
        ),
        env = env_dict
    )
    
    logging.info(f'Starting the dbt transformation task')
    dbt_transform_cverawstable = BashOperator(
        task_id = 'cve_raws_transformation',
        bash_command = (
            'cd /opt/airflow/repo ; '
            'dbt build --project-dir dbt --profiles-dir dbt --select sources'
        )
    )

    # Defining the dependencies
    extract_raw_jsons  >> load_raw_json >> dbt_transform_cverawstable 